<%
  groups = OodSupport::User.new.groups.map(&:name).select { |word| word.start_with?('p', 'b', 'a') }

  # your image location will differ
  ppn = num_cores.blank? ? 1 : num_cores.to_i
  walltime = (bc_num_hours.to_i * 60)
%>
---
batch_connect:
  template: basic
  websockify_cmd: '/usr/bin/websockify'
  script_wrapper: |
    module purge
    module load singularity
    cat << "CTRSCRIPT" > container.sh
    export PATH="$PATH:/opt/TurboVNC/bin"
    %s  
    CTRSCRIPT

    export container_image=/software/openondemand/jupyter/jupyter-ood.sif

    # Benchmark info
    echo "TIMING - Starting jupyter at: $(date)"

    # Launch the Jupyter Notebook Server

    JUPYTER_TMPDIR=${TMPDIR:-/tmp}/$(id -un)-jupyter/
    ## for conda
    export SINGULARITYENV_CONDA_PKGS_DIRS=${HOME}/.conda/pkgs
    ## for pip
    export SINGULARITYENV_XDG_CACHE_HOME=${JUPYTER_TMPDIR}/xdg_cache_home
    ## job specific tmp in case there are conflicts with different users running on the same node
    WORKDIR=${HOME}/jupyter/${SLURM_JOB_ID}

    mkdir -m 700 -p ${WORKDIR}/tmp

    ## for specific package mne
    export SINGULARITYENV_NUMBA_CACHE_DIR=$HOME/.cache

    export SING_GPU="--nv"

    # All our software in /software so have to bind that
    export SING_BINDS="--bind /software:/software"

    ## bind some extra stuff to be able to talk to slurm from within the container
    export SING_BINDS="$SING_BINDS -B /etc/passwd -B /etc/slurm -B `which sbatch ` -B `which srun ` -B `which sacct ` -B `which scontrol ` -B `which salloc ` -B /usr/lib64/slurm/ -B /usr/lib64/libmunge.so.2 -B /usr/lib64/libmunge.so.2.0.0 "

    # only bind /kellogg is individual is part of kellogg group
    export SING_BINDS="$SING_BINDS <%= groups.include?('kellogg') ? "--bind /kellogg/:/kellogg/" : "" %>"

    # only bind /scratch/<netid> if individual has a scratch space
    export SING_BINDS="$SING_BINDS <%= File.exist?("/scratch/#{User.new.name}") ? "--bind /scratch/#{User.new.name}:/scratch/#{User.new.name}" : "" %>"

    # Only bind projects directories that the user would have access to based on their unix groups
    <%- groups.each do |group| %>
    export SING_BINDS="$SING_BINDS --bind /projects/<%= group %>:/projects/<%= group %>"
    <%- end %>

    ## job specific tmp dir
    export SING_BINDS=" $SING_BINDS -B ${WORKDIR}/tmp:/tmp " 

    singularity exec $SING_GPU $SING_BINDS "${container_image}" /bin/bash container.sh

  header: | 
    #!/bin/bash
        . ~/.bashrc
script:
   email_on_started: true
   native:
     - "--partition"
     - "<%= slurm_partition %>"
     - "--account"
     - "<%= slurm_account %>"
     # How much time (in hours)
     - "--time"
     - "<%= walltime %>"
     # How many nodes (always 1)
     - "--nodes"
     - "1"
     # How many CPUs
     - "--ntasks-per-node"
     - "<%= ppn %>"
     # How much memory
     - "--mem"
     - "<%= memory_per_node %>G"
     # Job Name
     - "--job-name"
     - "<%= job_name %>"
     <%- if user_email != "" %>
     - "--mail-user"
     - "<%= user_email %>"
     <%- end %>
     <%- if gres_value != "" %>
     - "--gres"
     - "<%= gres_value %>"
     <%- end %>
